flows:
  parse-url-definition:
    - script: groovy
      body: |
        import java.io.UnsupportedEncodingException
        
        import java.net.URI
        import java.net.URISyntaxException
        import java.net.URLDecoder
        
        import java.nio.charset.StandardCharsets
        
        import java.time.Duration
        
        import java.util.Collections
        import java.util.HashMap
        
        import java.util.function.Function
        import java.util.regex.Pattern
        import java.util.stream.Collectors
        
        import org.apache.kafka.clients.consumer.KafkaConsumer
        import org.apache.kafka.clients.producer.KafkaProducer
        import org.apache.kafka.clients.producer.ProducerRecord
        
        import org.apache.kafka.common.TopicPartition
        
        def PARAM_SEPARATOR = "&"
        def VALUE_SEPARATOR = "="
        def ENCODED_NULL = "-null-"
        
        def uri = "${replaySource.sourceUrl}"
        def environment = "${replaySource.environment}"
        
        def uriToParse = applySubstitutions(uri)
        def schema = parseSchemaFromDefinition(uri)
        
        switch(schema) {
          case 'kafka':
            def parameters = extractParameters(PARAM_SEPARATOR,VALUE_SEPARATOR,uri)
            def topic = getSourceTopic()
            def sinkTopic = getSinkTopic() 
            def host = resolveHost(schema, environment)
        
            //def consumer = createKafkaConsumer(host)
            //def totalNumberOfMessages = getMessageCountForTopic(consumer, topic)
            //def filteredMessages = filterMessagesToReplay(consumer, topic, totalNumberOfMessages)
            //consumer.close()
            
            //replayKafkaMessages(host, filteredMessages, sinkTopic)
            break;
          default:
            println('Schema ' + schema + ' is not supported .....' )
            break;
        }
        
        def replayKafkaMessages(host, messages, topic) {
          def producer = createKafkaProducer(host) 
          messages.each { message -> 
            def producerRecord = new ProducerRecord(topic, message.partition(), message.key(), message.value(), message.headers())
            producer.send(producerRecord)
          }
          producer.close()
        }
        
        def filterKafkaMessagesToReplay(consumer, topic, totalNumberOfMessages) {
          def numberOfMessagesReadSoFar = 0
          def keepOnReading = true
          def filteredMessages = new ArrayList<>();
        
          def partitionToReadFrom = new TopicPartition(topic, 0)
          consumer.assign(List.of(partitionToReadFrom));
          consumer.seek(partitionToReadFrom, 0L)
        
          while(keepOnReading) {
            if ( totalNumberOfMessages == 0 ) {
                keepOnReading = false;
            } else {
              def records = consumer.poll(Duration.ofMillis(500)) 
              records.each { record ->
                numberOfMessagesReadSoFar += 1
                if ( containsMessageIdFromHeader(record) ) {
                  filteredMessages.add(record)
                }
                
                if (numberOfMessagesReadSoFar >= totalNumberOfMessages) {
                  keepOnReading = false;
                }
              }
            }
          }
          return filteredMessages
        }
        
        def containsMessageIdFromHeader(consumerRecord) {
          def headerFilterExists = false
          consumerRecord.headers().each { header ->
            if ( header.key().equals('messageId') ) {
              def messageId = new String(header.value(), StandardCharsets.UTF_8).trim();
              if(messageIdentifiers.contains(messageId)) {
                headerFilterExists = true
              }
            }
          }
          return headerFilterExists
        }
        
        def getMessageCountForKafkaTopic(consumer, topic) {
          def partitions = consumer.partitionsFor(topic).
              stream().
              map(p -> new TopicPartition(topic, p.partition())).
              collect(Collectors.toList())
          consumer.assign(partitions);
          consumer.seekToEnd(Collections.emptySet());
          def endPartitions = partitions.
              stream().
              collect(Collectors.toMap(Function.identity(), consumer::position));
          return partitions.stream().mapToLong(p -> endPartitions.get(p)).sum()
        }
        
        def createKafkaProducer(host) {
          def producer = new KafkaProducer([
              "bootstrap.servers" : host,
              "value.serializer"  : "org.apache.kafka.common.serialization.ByteArraySerializer",
              "key.serializer"  : "org.apache.kafka.common.serialization.ByteArraySerializer",
          ])
          return producer
        }
        
        def createKafkaConsumer(host) {
          def consumer = new KafkaConsumer([
              "bootstrap.servers"      : host,
              "group.id"               : "hawkeye-replay-consumer",
              "enable.auto.commit"     : "false",
              "value.deserializer"     : "org.apache.kafka.common.serialization.ByteArrayDeserializer",
              "key.deserializer"       : "org.apache.kafka.common.serialization.ByteArrayDeserializer"
            ]) 
            return consumer
        }
        
        def getSourceTopic() {
          def splitSource = replaySource.sourceUrl.split('://')
          return splitSource[1].split('/')[1]
        }
        
        def getSinkTopic() {
          def splitSink = replaySource.sinkUrl.split('://')
          return splitSink[1].split('/')[1] 
        }
        
        def isProductionEnvironment(environment) {
          if(environment == 'prod') {
            return true
          }
          return false
        }
        
        def resolveHost(schema, environment) {
          switch(schema) {
            case 'kafka':
              if(isProductionEnvironment(environment)) {
                  return "10.0.0.121:9092"
              } else {
                  return "10.0.0.121:9092" 
              }
              break;
            case 'cosmosdb':
              if(isProductionEnvironment(environment)) {
                  return "" 
              } else {
                  return "" 
              }
              break;
            default:
              println('Schema ' + schema + ' is not supported .....' )
              break; 
          }
        }
        
        def extractParameters(paramSeparator, valueSeparator, uri) {
          def parameters = new HashMap<>()
          def allUriParams = URI.create(uri).getRawQuery()
          if( allUriParams == null ) {
            return parameters
          } else {
            allUriParams.split(paramSeparator).each { parameter -> 
              def keyValuePair = parameter.split(valueSeparator, 2)
              map.put(keyValuePair[0], decode(keyValuePair[1])
            }
            return parameters
          }
        }
        
        def decode(valueToDecode) {
          try {
            def decoded = URLDecoder.decode(valueToDecode, StandardCharsets.UTF_8.toString())
            return ENCODED_NULL.equals(decoded) ? null : decoded
          } catch ( UnsupportedEncodingException eX ) {
            return valueToDecode
          }
        }
      
        def parseSchemaFromDefinition(uri) {
          def splitSource = uri.split('://')
          return splitSource[0]
        }
        
        def applySubstitutions(uri) {
            def pattern = Pattern.compile("\\{([^{}]+)\\}"); 
            def matcher = pattern.matcher(uri)
        
            def substitutedConfig = uri
            while(matcher.find()) {
              def group = matcher.group()
              if(!group.equals("{}")) {
                def key = group.substring(1,group.length() - 1)
                println "Key value to lookup is: ${key}\n"
              }
            }
        }
