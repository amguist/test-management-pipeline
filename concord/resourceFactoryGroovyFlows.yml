flows:
  parse-url-definition:
    - script: groovy
      body: |
        import com.google.gson.GsonBuilder
        
        import java.nio.charset.StandardCharsets
        
        import java.time.Duration
        
        import java.util.Collections
        
        import java.util.function.Function
        import java.util.regex.Pattern
        import java.util.stream.Collectors
        
        import org.apache.kafka.clients.consumer.KafkaConsumer
        import org.apache.kafka.clients.producer.KafkaProducer
        import org.apache.kafka.clients.producer.ProducerRecord
        
        import org.apache.kafka.common.TopicPartition
        
        def uri = "${replaySource.sourceUrl}"
        def environment = "${replaySource.environment}"
        
        def uriToParse = applySubstitutions(uri)
        def schema = parseSchemaFromDefinition(uri)
        
        switch(schema) {
          case 'kafka':
            def host = resolveHost(schema, environment)
            def consumer = createKafkaConsumer(host)
            def topic = getSourceTopic()
            def totalNumberOfMessages = getMessageCountForTopic(consumer, topic)
            def filteredMessages = filterMessagesToReplay(consumer, topic, totalNumberOfMessages)
            consumer.close()
       
            def sinkTopic = getSinkTopic() 
            replayMessages(host, filteredMessages, sinkTopic)
            
            break;
          case 'cosmosdb':
            println('Schema is of type Cosmos DB ....')
            def host = resolveHost(schema, environment)
            break;
          default:
            println('Schema ' + schema + ' is not supported .....' )
            break;
        }
        
        def replayMessages(host, messages, topic) {
          def producer = createKafkaProducer(host) 
          def gson = new GsonBuilder().serializeNulls().setPrettyPrinting().create()
          messages.each { message -> 
            def headers = message.headers()
            def messageValue = new String(message.value(), StandardCharsets.UTF_8).trim();
        
            producer.send(new ProducerRecord(topic, messageValue))
          }
          producer.close()
        }
        
        def filterMessagesToReplay(consumer, topic, totalNumberOfMessages) {
          def numberOfMessagesReadSoFar = 0
          def keepOnReading = true
          def filteredMessages = new ArrayList<>();
        
          def partitionToReadFrom = new TopicPartition(topic, 0)
          consumer.assign(List.of(partitionToReadFrom));
          consumer.seek(partitionToReadFrom, 0L)
        
          while(keepOnReading) {
            if ( totalNumberOfMessages == 0 ) {
                keepOnReading = false;
            } else {
              def records = consumer.poll(Duration.ofMillis(500)) 
              records.each { record ->
                numberOfMessagesReadSoFar += 1
                if ( containsMessageIdFromHeader(record) ) {
                  filteredMessages.add(record)
                }
                
                if (numberOfMessagesReadSoFar >= totalNumberOfMessages) {
                  keepOnReading = false;
                }
              }
            }
          }
          return filteredMessages
        }
        
        def containsMessageIdFromHeader(consumerRecord) {
          def headerFilterExists = false
          consumerRecord.headers().each { header ->
            if ( header.key().equals('messageId') ) {
              def messageId = new String(header.value(), StandardCharsets.UTF_8).trim();
              if(messageIdentifiers.contains(messageId)) {
                headerFilterExists = true
              }
            }
          }
          return headerFilterExists
        }
        
        def getMessageCountForTopic(consumer, topic) {
          def partitions = consumer.partitionsFor(topic).
              stream().
              map(p -> new TopicPartition(topic, p.partition())).
              collect(Collectors.toList())
          consumer.assign(partitions);
          consumer.seekToEnd(Collections.emptySet());
          def endPartitions = partitions.
              stream().
              collect(Collectors.toMap(Function.identity(), consumer::position));
          return partitions.stream().mapToLong(p -> endPartitions.get(p)).sum()
        }
        
        def createKafkaProducer(host) {
          def producer = new KafkaProducer([
              "bootstrap.servers"      : host,
              "value.serializer"     : "org.apache.kafka.common.serialization.ByteArraySerializer",
              "key.serializer"       : "org.apache.kafka.common.serialization.ByteArraySerializer" 
          ])
          return producer
        }
        
        def createKafkaConsumer(host) {
          def consumer = new KafkaConsumer([
              "bootstrap.servers"      : host,
              "group.id"               : "hawkeye-replay-consumer",
              "enable.auto.commit"     : "false",
              "value.deserializer"     : "org.apache.kafka.common.serialization.ByteArrayDeserializer",
              "key.deserializer"       : "org.apache.kafka.common.serialization.ByteArrayDeserializer"
            ]) 
            return consumer
        }
        
        def getSourceTopic() {
          def splitSource = replaySource.sourceUrl.split('://')
          return splitSource[1].split('/')[1]
        }
        
        def getSinkTopic() {
          def splitSink = replaySource.sinkUrl.split('://')
          return splitSink[1].split('/')[1] 
        }
        
        def isProductionEnvironment(environment) {
          if(environment == 'prod') {
            return true
          }
          return false
        }
        
        def resolveHost(schema, environment) {
          switch(schema) {
            case 'kafka':
              if(isProductionEnvironment(environment)) {
                  return "10.0.0.121:9092"
              } else {
                  return "10.0.0.121:9092" 
              }
              break;
            case 'cosmosdb':
              if(isProductionEnvironment(environment)) {
                  return "" 
              } else {
                  return "" 
              }
              break;
            default:
              println('Schema ' + schema + ' is not supported .....' )
              break; 
          }
        }
      
        def parseSchemaFromDefinition(uri) {
          def splitSource = uri.split('://')
          return splitSource[0]
        }
        
        def applySubstitutions(uri) {
            def pattern = Pattern.compile("\\{([^{}]+)\\}"); 
            def matcher = pattern.matcher(uri)
        
            def substitutedConfig = uri
            while(matcher.find()) {
              def group = matcher.group()
              if(!group.equals("{}")) {
                def key = group.substring(1,group.length() - 1)
                println "Key value to lookup is: ${key}\n"
              }
            }
        }
